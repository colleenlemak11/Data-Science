{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height(cm)  weight(kg) t-shirt size\n",
      "0         158          58            M\n",
      "1         163          61            M\n",
      "2         165          61            L\n",
      "3         168          66            L\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"shirt_sizes.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML (Machine Learning)\n",
    "* Supervised learning: labeled data (ex: there is an attribute (AKA feature) that you are interested in predicting for unseen instances)\n",
    "    * The attribute is called the \"class\" or the \"class label\"\n",
    "    * The attribute is categorical...classification\n",
    "    * The attribute is numeric...regression\n",
    "    * Example algorithm: kNN (k nearest neighbors)\n",
    "* Unsupervised learning: unlabeled data \n",
    "    * Example algorithm: k-means clustering\n",
    "\n",
    "## Supervised Learning\n",
    "* Need a way to divide a dataset into a training set and a testing set\n",
    "    * The training set is used to build/train an algorithm/model\n",
    "    * The testing set is used to evaluate the algorithm/model\n",
    "    * The training set and the testing set *are different*\n",
    "* Example\n",
    "    * We have this super tiny t-shirt sizes dataset\n",
    "        * 4 instances\n",
    "        * 3 attributes (1 is the class which is t-shirt size)\n",
    "        * Goal is to use height and weight attributes to predict t-shirt size\n",
    "        * We will do this for a test set with a single unseen instance\n",
    "            * height=161 weight=63 t-shirt size=?\n",
    "            * Let's say the \"ground truth value\" is M (medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Algorithm\n",
    "* Identify the k nearest neighbors in the training set to a test set instance\n",
    "    * The most frequently occurring class label amongst the k nearest neighbors will be the class label prediction for the unseen instance\n",
    "* We need a way to measure \"nearness\" AKA \"closeness\"\n",
    "    * 2D: Pythagorean theorem\n",
    "    * ND: Euclidean distance formula = $dist(a,b) = \\sqrt{\\sum_{i=1}^{n} {(a_i-b_i)^2}}$\n",
    "* We need to normalize (AKA scale) our attributes so we don't have an unanticipated weighting of one attribute more than another (ex: height has a larger scale than weight, so it will dominate the formula)\n",
    "    * We will use the min-max scaling approach\n",
    "    * For each attribute, the min becomes 0 and the max becomes 1 (ex: bounded to [0,1] so the units have no weighting effect)\n",
    "    * For each attribute, for each value subtract the min then divide by the original range (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   height(cm)  weight(kg)\n",
      "0         158          58\n",
      "1         163          61\n",
      "2         165          61\n",
      "3         168          66\n",
      "0    M\n",
      "1    M\n",
      "2    L\n",
      "3    L\n",
      "Name: t-shirt size, dtype: object\n",
      "[[161, 63]]\n"
     ]
    }
   ],
   "source": [
    "# kNN algorithm with the sci-kit learn library\n",
    "# notation\n",
    "# X: is a feature matrix (rows of feature vectors (instances)) with the class labels stripped off\n",
    "# y: is a class label vector\n",
    "# X and y are parallel\n",
    "# use _train and _test to denote train and test sets respectively\n",
    "X_train = df.drop(\"t-shirt size\", axis=1) # 1 is for columns\n",
    "print(X_train)\n",
    "y_train = df[\"t-shirt size\"]\n",
    "print(y_train)\n",
    "X_test = [[161, 63]]\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm steps:\n",
    "1. Normalize the X data\n",
    "2. Compute the distances to each unseen instance in the test set\n",
    "3. Apply majority voting to k(=3 in our case) closest distances' labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.   ]\n",
      " [0.5   0.375]\n",
      " [0.7   0.375]\n",
      " [1.    1.   ]]\n",
      "[[0.3   0.625]]\n"
     ]
    }
   ],
   "source": [
    "# normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train) # often combined fit_transform()\n",
    "print(X_train_normalized)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "print(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='euclidean', n_neighbors=3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up knn classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3, metric=\"euclidean\")\n",
    "knn_clf.fit(X_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y predicted: ['M']\n",
      "nearest neighbors: (array([[0.32015621, 0.47169906, 0.69327123]]), array([[1, 2, 0]], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# to make predictions with the classifier for unseen instance\n",
    "y_predicted = knn_clf.predict(X_test_normalized)\n",
    "print(\"y predicted:\", y_predicted)\n",
    "print(\"nearest neighbors:\", knn_clf.kneighbors(X_test_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts on kNN\n",
    "* What if our attributes are not numeric (meaning they are categorical?)\n",
    "    * Simple approach: convert category labels to integers\n",
    "        * from sklearn.preprocessing import LabelEncoder\n",
    "    * Another approach: write your own distance function (0 if the labels are the same, 1 otherwise)\n",
    "* kNN is not the only ML algorithm\n",
    "    * Naive Bayes\n",
    "    * Decision trees (random forests)\n",
    "    * SVMs (support vector machines)\n",
    "    * Neural networks\n",
    "    * etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nov 30, 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.         0.1       ]\n",
      " [0.         0.5       ]\n",
      " [0.16666667 0.1       ]\n",
      " [0.16666667 0.2       ]\n",
      " [0.41666667 0.2       ]\n",
      " [0.41666667 0.3       ]\n",
      " [0.16666667 0.6       ]\n",
      " [0.41666667 0.6       ]\n",
      " [0.58333333 0.3       ]\n",
      " [0.58333333 0.4       ]\n",
      " [0.58333333 0.7       ]\n",
      " [0.83333333 0.4       ]\n",
      " [0.83333333 0.5       ]\n",
      " [0.83333333 0.8       ]\n",
      " [1.         0.5       ]\n",
      " [1.         0.6       ]\n",
      " [1.         1.        ]]\n",
      "0     M\n",
      "1     M\n",
      "2     M\n",
      "3     M\n",
      "4     M\n",
      "5     M\n",
      "6     M\n",
      "7     L\n",
      "8     L\n",
      "9     L\n",
      "10    L\n",
      "11    L\n",
      "12    L\n",
      "13    L\n",
      "14    L\n",
      "15    L\n",
      "16    L\n",
      "17    L\n",
      "Name: t-shirt size, dtype: object\n"
     ]
    }
   ],
   "source": [
    "long_df = pd.read_csv(\"shirt_sizes_large.csv\")\n",
    "\n",
    "X = long_df.drop(\"t-shirt size\", axis=1) # 1 is for columns\n",
    "y = long_df[\"t-shirt size\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Evaluation\n",
    "* In our previous demo, we had 1 instance in our \"test set\"\n",
    "    * If our classifier predicted this instance's class correctly, accuracy 100%\n",
    "    * If our classifier predicted this instance's class incorrectly, accuracy 0%\n",
    "* Note\n",
    "    * We should use a \"large\" test set to get a better picture of how our classifier is performing\n",
    "    * Accuracy doesn't tell the whole story...\n",
    "        * Ex: 100 samples... 99 M and 1 L\n",
    "        * And our classifier simply only predicts M\n",
    "        * We have 99% accuracy yay!\n",
    "            * Accuracy only makes sense when your class labels are near evenly distributed\n",
    "\n",
    "* Given a data set, we need a way to \"divide\" our dataset into a training set and a test set\n",
    "    * A few ways to use this\n",
    "        1. Hold out method\n",
    "        2. Random subsampling\n",
    "        3. Cross validation\n",
    "        4. Boot-strap method\n",
    "\n",
    "### Hold out method\n",
    "1. \"Hold out\" a certain number or percentage of instances in a dataset for testing\n",
    "    * Train on the remaining instances\n",
    "    * Typically choose a standard split or percentage\n",
    "        * ex: 2:1 split means 1/3 of data is held out for testing, so remaining 2/3 for training\n",
    "        * ex: 25% hold out -- 25% of data is held out for testing and 75% is for training\n",
    "            * Default for sklearn's `train_test_split()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'M' 'L' 'L' 'L']\n",
      "['M', 'M', 'L', 'L', 'L']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# random state is used for reproducablility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) #  random_state=0\n",
    "# print(X_train, X_test)\n",
    "# print(y_train, y_test)\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3, metric=\"euclidean\")\n",
    "knn_clf.fit(X_train, y_train)\n",
    "y_predicted = knn_clf.predict(X_test)\n",
    "print(y_predicted)\n",
    "print(list(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'M' 'L' 'L' 'L']\n",
      "['M', 'M', 'L', 'L', 'L']\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_predicted = tree_clf.predict(X_test)\n",
    "print(y_predicted)\n",
    "print(list(y_test))\n",
    "\n",
    "accuracy = knn_clf.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Subsampling\n",
    "* Performing the hold out method k times (different from kNN)\n",
    "* Accuracy is the mean accuracy over the k runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "* With a random subsampling, we are not guarenteed that each instance ends up in a test set at least once\n",
    "* With cross validaiton, we are more intentional about our \"partitions\"\n",
    "* Algorithm: Divide the data set into k folds (also different k)\n",
    "    * For each fold:\n",
    "        * Hold out the fold and test on it\n",
    "        * Train on the remaining folds (folds - fold)\n",
    "* With this algorithm, each instance is tested exactly 1 time\n",
    "* Accuracy is the total predicted correctly divided by the total predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neighbors._classification.KNeighborsClassifier'>\n",
      "[0.75       0.5        1.         1.         0.66666667] 0.7833333333333333\n",
      "['M' 'M' 'M' 'M' 'M' 'M' 'L' 'M' 'L' 'M' 'M' 'L' 'L' 'L' 'L' 'L' 'L' 'L']\n",
      "0.7777777777777778\n",
      "<class 'sklearn.tree._classes.DecisionTreeClassifier'>\n",
      "[0.5        0.5        1.         1.         0.66666667] 0.7333333333333333\n",
      "['M' 'M' 'L' 'M' 'M' 'M' 'L' 'M' 'M' 'M' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L']\n",
      "0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "# run 5-fold cross validation for both the knn and tree\n",
    "for clf in [knn_clf, tree_clf]:\n",
    "    print(type(clf))\n",
    "    # lazier approach\n",
    "    accuracies = cross_val_score(clf, X, y, cv=5)\n",
    "    print(accuracies, accuracies.mean())\n",
    "    # better approach\n",
    "    y_predicted = cross_val_predict(clf, X, y, cv=5)\n",
    "    print(y_predicted)\n",
    "    accuracy = accuracy_score(y, y_predicted)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GS for next class...\n",
    "\n",
    "NOTE: from cross_val_score() documentation: \"For int/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. These splitters are instantiated with shuffle=False so the splits will be the same across calls.\"\n",
    "\n",
    "TODO: cross validation variants plus confusion matrices\n",
    "\n",
    "Variants of cross-validation\n",
    "* Stratified k fold cross validation: roughly the same distribution of class labels in each fold\n",
    "* LOOCV: leave one out cross validation: k = N: each fold contains exactly one instance\n",
    "    * Good for when you need as much training data as possible\n",
    "    * Inefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Evaluation Metrics\n",
    "For binary classification...\n",
    "* P: # of positive instances in the test set\n",
    "* N: # of negative instances in the test set\n",
    "* TP: # of positives that are correctly classified as positives (true positives)\n",
    "* TN: # of negatives that are correctly classified as negatives\n",
    "* FN: # of positives that are incorrectly classified as negatives\n",
    "* FP: # of negatives that are incorrectly classified as positives\n",
    "* Accuracy = $\\frac{TP + TN}{P + N}$\n",
    "* Error rate = $\\frac{FN + FP}{P + N}$\n",
    "* Accuracy and error rate are not always the best metrics\n",
    "    * Especially when you have a class imbalance problem\n",
    "* Other metrics (for further study)\n",
    "    * Precision, recall, F measure, AUC (area under the receiver operator curve), etc...\n",
    "\n",
    "Last ... confusion matrix demo"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62d5fa2a16ce955d7a264e6706feb8088754ba4a8b2fa506b19effc7ba754be8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
